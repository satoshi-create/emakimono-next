# 04_rag_chat.py
# âœ… OpenAI APIä¸è¦ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹å–„ç‰ˆï¼‰

import faiss
import json
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline, GPT2Tokenizer

# ========================
# ğŸ”¹ 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
# ========================
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
language_model = pipeline("text-generation", model="distilgpt2")
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2ã«ã¯pad_tokenãŒãªã„ãŸã‚eosã§ä»£ç”¨

# ========================
# ğŸ”¹ 2. åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# ========================
def load_jsonl_embeddings(path):
    entries = []
    vectors = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            obj = json.loads(line)
            entries.append(obj)
            vectors.append(np.array(obj['embedding'], dtype='float32'))
    return entries, np.vstack(vectors)

# ========================
# ğŸ”¹ 3. FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿é–¢æ•°
# ========================
def load_faiss_index(index_path):
    return faiss.read_index(index_path)

# ========================
# ğŸ”¹ 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¨ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
# ========================
query = input("ğŸ“¥ è³ªå•ã‚’ã©ã†ãï¼š")
query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')

# ========================
# ğŸ”¹ 5. ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
# ========================
entries, _ = load_jsonl_embeddings(os.path.join("..", "data/embeddings", "kou_embedding_ready.jsonl_sentences.jsonl"))
index = load_faiss_index(os.path.join("..", "data/embeddings", "faiss.index"))

# ========================
# ğŸ”¹ 6. FAISSé¡ä¼¼æ¤œç´¢
# ========================
D, I = index.search(query_embedding, k=3)

# ========================
# ğŸ”¹ 7. é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆé–¾å€¤ã‚’è¶…ãˆã‚‹ã¨ç„¡åŠ¹ï¼‰
SIMILARITY_THRESHOLD = 0.7  # cosineè·é›¢ï¼ˆå°ã•ã„ã»ã©è¿‘ã„ï¼‰
if D[0][0] > SIMILARITY_THRESHOLD:
    print("\nâš ï¸ è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    exit()

print(f"ğŸ§ª æœ€ã‚‚è¿‘ã„è·é›¢ã‚¹ã‚³ã‚¢: {D[0][0]:.4f}")

# ========================
# ğŸ”¹ 8. æ¤œç´¢çµæœã®æ–‡è„ˆç”Ÿæˆã¨ãƒã‚§ãƒƒã‚¯
# ========================
retrieved_texts = [entries[i]['text'] for i in I[0]]
context = "\n\n".join(retrieved_texts)

if len(context.strip()) < 100:
    print("\nâš ï¸ é–¢é€£ã™ã‚‹ååˆ†ãªæ–‡è„ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    exit()

# ========================
# ğŸ”¹ 9. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆã‚ã‹ã‚‰ãªã„æ™‚ã®å¿œç­”ã‚‚æŒ‡å®šï¼‰
# ========================
prompt = f"""æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
ãŸã ã—ã€æ–‡è„ˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œãã®ä»¶ã«ã¤ã„ã¦ã¯æ–‡è„ˆã«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆ:
{context}

è³ªå•:
{query}

ç­”ãˆï¼š"""

# ========================
# ğŸ”¹ 10. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆ1024æœªæº€ï¼‰ã«èª¿æ•´
# ========================
MAX_TOKENS = 800
inputs = tokenizer(prompt, truncation=True, max_length=MAX_TOKENS, return_tensors="pt")
trimmed_prompt = tokenizer.decode(inputs["input_ids"][0])

# ========================
# ğŸ”¹ 11. å›ç­”ç”Ÿæˆ
# ========================
result = language_model(trimmed_prompt, max_new_tokens=200, do_sample=False)[0]['generated_text']

# ========================
# ğŸ”¹ 12. å‡ºåŠ›
# ========================
print("\nğŸ’¡ å›ç­”:")
print(result)
