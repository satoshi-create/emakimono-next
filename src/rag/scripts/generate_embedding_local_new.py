import json
from sentence_transformers import SentenceTransformer
from transformers import GPT2Tokenizer

# âœ… äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
# åŸ‹ã‚è¾¼ã¿ç”¨ã®è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆMiniLMï¼‰ã‚’ä½¿ã†
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆæ¸¬ã®ãŸã‚ã«ã€GPT2ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")

# âœ… å…¥åŠ›ï¼šã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†å‰²ï¼‹ãƒ¡ã‚¿æƒ…å ±ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‰ã‚¹ãƒ†ãƒƒãƒ—ã§ç”Ÿæˆï¼‰
input_file = "../data/jsonl/kou_split_metadata.jsonl"

# âœ… å‡ºåŠ›ï¼šembeddingã¨tokenæ•°ã‚’ä»˜åŠ ã—ãŸå®Œå…¨ãªRAGå…¥åŠ›å½¢å¼
output_file = "kou_embedding_ready.jsonl_sentences.jsonl"

# å„è¡Œã«å¯¾ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
with open(input_file, "r", encoding="utf-8") as fin, \
     open(output_file, "w", encoding="utf-8") as fout:

    for line in fin:
        item = json.loads(line)  # JSONå½¢å¼ã§èª­ã¿è¾¼ã¿

        # ğŸ”¹ Step 1: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆæ¸¬ï¼ˆpromptã®é•·ã•èª¿æ•´ç”¨ï¼‰
        item["tokens"] = len(tokenizer.encode(item["text"]))

        # ğŸ”¹ Step 2: SentenceTransformerã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆ384æ¬¡å…ƒï¼‰
        item["embedding"] = embedder.encode(item["text"]).tolist()

        # ğŸ”¹ Step 3: æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãå‡ºã—
        fout.write(json.dumps(item, ensure_ascii=False) + "\n")

print("âœ… kou_with_embeddings_sentences.jsonl ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
