# rag_chat_fix_final.py
# âœ… ãƒ­ãƒ¼ã‚«ãƒ«RAGï¼šFAISSãƒ™ãƒ¼ã‚¹ã®æ–‡è„ˆæ¤œç´¢ + distilgpt2ã«ã‚ˆã‚‹ç”Ÿæˆ
# âœ… ãƒã‚°å¯¾ç­–ï¼šå¿œç­”å´©å£Šï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—ã‚„ï¼šï¼šï¼šç¹°è¿”ã—ï¼‰é˜²æ­¢ + debugå‡ºåŠ›

import faiss
import json
import os
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline, GPT2Tokenizer

# ========================
# ğŸ”¹ 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
# ========================
# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¢ãƒ‡ãƒ«ï¼šMiniLMï¼ˆ384æ¬¡å…ƒï¼‰
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼šdistilgpt2ï¼ˆè»½é‡GPT2ï¼‰
language_model = pipeline("text-generation", model="distilgpt2")

# GPT2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆpad_tokenãŒãªã„ãŸã‚ã€eos_tokenã‚’padã¨ã—ã¦ä»£ç”¨ï¼‰
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

# ========================
# ğŸ”¹ 2. JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã¨æ–‡æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
# ========================
def load_jsonl_embeddings(path):
    entries = []  # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿æƒ…å ±
    vectors = []  # FAISSç”¨ãƒ™ã‚¯ãƒˆãƒ«
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            obj = json.loads(line)
            entries.append(obj)
            vectors.append(np.array(obj['embedding'], dtype='float32'))
    return entries, np.vstack(vectors)

# ========================
# ğŸ”¹ 3. FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
# ========================
def load_faiss_index(index_path):
    return faiss.read_index(index_path)

# ========================
# ğŸ”¹ 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è³ªå•ã‚’å—ã‘å–ã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–
# ========================
query = input("ğŸ“¥ è³ªå•ã‚’ã©ã†ãï¼š")
query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')

# ========================
# ğŸ”¹ 5. æ–‡ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰
# ========================
entries, _ = load_jsonl_embeddings(os.path.join("..", "data/embeddings", "kou_embedding_ready.jsonl_sentences.jsonl"))
index = load_faiss_index(os.path.join("..", "data/embeddings", "faiss.index"))

# ========================
# ğŸ”¹ 6. ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦é¡ä¼¼æ–‡ã‚’ä¸Šä½5ä»¶æ¤œç´¢ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼ï¼‰
# ========================
D, I = index.search(query_embedding, k=5)  # D: é¡ä¼¼ã‚¹ã‚³ã‚¢, I: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·

# ========================
# ğŸ”¹ 7. é¡ä¼¼åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆé–¾å€¤ä»¥ä¸‹ã¯ç„¡è¦–ã€‚ãªã‘ã‚Œã°fallbackï¼‰
# ========================
SIMILARITY_THRESHOLD = 0.5
retrieved = [(entries[idx]['text'], D[0][j]) for j, idx in enumerate(I[0]) if D[0][j] >= SIMILARITY_THRESHOLD]

# ã‚¹ã‚³ã‚¢ãŒå…¨ã¦é–¾å€¤æœªæº€ãªã‚‰ã€æœ€ã‚‚è¿‘ã„1ä»¶ã‚’å¼·åˆ¶æ¡ç”¨
if not retrieved:
    fallback_idx = I[0][0]
    retrieved = [(entries[fallback_idx]['text'], D[0][0])]
    print("âš ï¸ é–¾å€¤ã‚’ä¸‹å›ã£ãŸãŸã‚æœ€ã‚‚è¿‘ã„æ–‡ã‚’1ä»¶ä½¿ç”¨ã—ã¾ã™ã€‚")

# ========================
# ğŸ”¹ 8. æ¤œç´¢æ–‡è„ˆã‹ã‚‰é•·ã•åˆ¶é™ä»˜ãã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
# ========================
MAX_CONTEXT_TOKENS = 800
context_sentences = []
total_tokens = 0

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆè¨ˆãŒä¸Šé™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«æ–‡ã‚’è¿½åŠ 
for text, _ in retrieved:
    entry = next((e for e in entries if e["text"] == text), None)
    if entry and "tokens" in entry:
        if total_tokens + entry["tokens"] <= MAX_CONTEXT_TOKENS:
            context_sentences.append(text)
            total_tokens += entry["tokens"]

context = "\n\n".join(context_sentences)

# ========================
# ğŸ”¹ 9. RAGç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆGPT2ã«æ¸¡ã™æŒ‡ç¤ºæ–‡ï¼‰
# ========================
prompt = f"""ã‚ãªãŸã¯é³¥ç£æˆ¯ç”»ãƒ»ç”²å·»ã®å°‚é–€AIã§ã™ã€‚
æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
æƒ…å ±ãŒãªã‘ã‚Œã°ã€Œãã®ä»¶ã«ã¤ã„ã¦ã¯æ–‡è„ˆã«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆ:
{context}

è³ªå•:
{query}

ç­”ãˆï¼š"""

# ========================
# ğŸ”¹ 10. ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ãƒƒãƒˆï¼ˆæœ€å¤§1024åˆ¶é™ï¼‰
# ========================
inputs = tokenizer(prompt, truncation=True, max_length=1000, return_tensors="pt")
trimmed_prompt = tokenizer.decode(inputs["input_ids"][0])

# ========================
# ğŸ”¹ 11. å¿œç­”ç”Ÿæˆï¼ˆå®‰å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼‰
# ========================
result = language_model(
    trimmed_prompt,
    max_new_tokens=100,     # å¿œç­”ã¯æœ€å¤§100ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ç”Ÿæˆ
    do_sample=False,        # ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’ãªãã—å†ç¾æ€§ã‚’ç¢ºä¿
    pad_token_id=50256      # GPT2ç”¨ã®æ–‡æœ«ãƒˆãƒ¼ã‚¯ãƒ³
)[0]['generated_text']

# ========================
# ğŸ”¹ 12. å¿œç­”ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå´©å£Šãƒ»ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
# ========================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
generated_raw = result.replace(trimmed_prompt, "").strip()

# ã€Œç­”ãˆï¼šã€ä»¥é™ã ã‘æŠ½å‡ºï¼ˆä¸‡ä¸€è¤‡æ•°ã‚ã‚‹å ´åˆã‚‚å¯¾å¿œï¼‰
if "ç­”ãˆï¼š" in generated_raw:
    generated_clean = generated_raw.split("ç­”ãˆï¼š")[-1].strip()
else:
    generated_clean = generated_raw

# ä¸è‡ªç„¶ãªç¹°ã‚Šè¿”ã—ã‚„è¨˜å·ï¼ˆï¼šï¼šï¼šãªã©ï¼‰ã‚’æ­£è¦åŒ–
generated_clean = re.sub(r"[ï¼š:\.]{3,}", "ã€‚", generated_clean)
generated_clean = re.sub(r"ã€‚+", "ã€‚", generated_clean).strip()

# å¿œç­”ãŒçŸ­ã™ãã‚‹å ´åˆã€æœ€åˆã®æ–‡ã ã‘å–ã£ã¦è£œå®Œ
if len(generated_clean) < 5:
    generated_clean = generated_raw.split("ã€‚")[0] + "ã€‚"

# ========================
# ğŸ”¹ 13. ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆé¡ä¼¼æ–‡ã¨ã‚¹ã‚³ã‚¢ï¼‰
# ========================
DEBUG = True
if DEBUG:
    print("\n--- ğŸ” Debug info ---")
    for i, (text, score) in enumerate(retrieved):
        print(f"{i+1}. ({score:.4f}) {text[:60]}...")

# ========================
# ğŸ”¹ 14. å¿œç­”å‡ºåŠ›ï¼ˆæœ€çµ‚è¡¨ç¤ºï¼‰
# ========================
print("\nğŸ’¡ å›ç­”:")
print(generated_clean)
