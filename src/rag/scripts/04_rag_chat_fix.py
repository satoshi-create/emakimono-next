# 04_rag_chat.py
# âœ… OpenAI APIä¸è¦ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®‰å®šåŒ–ï¼‹ãƒ‡ãƒãƒƒã‚°å¯¾å¿œç‰ˆï¼‰

import faiss
import json
import os
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline, GPT2Tokenizer

# ========================
# ğŸ”¹ 1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
# ========================
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
language_model = pipeline("text-generation", model="distilgpt2")
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2ã«ã¯pad_tokenãŒãªã„ãŸã‚eosã§ä»£ç”¨

# ========================
# ğŸ”¹ 2. åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# ========================
def load_jsonl_embeddings(path):
    entries = []
    vectors = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            obj = json.loads(line)
            entries.append(obj)
            vectors.append(np.array(obj['embedding'], dtype='float32'))
    return entries, np.vstack(vectors)

# ========================
# ğŸ”¹ 3. FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿é–¢æ•°
# ========================
def load_faiss_index(index_path):
    return faiss.read_index(index_path)

# ========================
# ğŸ”¹ 4. ã‚¯ã‚¨ãƒªå…¥åŠ›ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
# ========================
query = input("ğŸ“¥ è³ªå•ã‚’ã©ã†ãï¼š")
query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')

# ========================
# ğŸ”¹ 5. ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰
# ========================
entries, vectors = load_jsonl_embeddings(os.path.join("..", "data/embeddings", "kou_embedding_ready.jsonl_sentences.jsonl"))
index = load_faiss_index(os.path.join("..", "data/embeddings", "faiss.index"))

# ========================
# ğŸ”¹ 6. FAISSæ¤œç´¢ï¼ˆä¸Šä½kä»¶ï¼‰
# ========================
D, I = index.search(query_embedding, k=5)

# ========================
# ğŸ”¹ 7. ã‚¹ã‚³ã‚¢é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆcosineé¡ä¼¼åº¦ï¼šé«˜ã„ã»ã©è¿‘ã„ï¼‰
SIMILARITY_THRESHOLD = 0.5
retrieved = [(entries[idx]['text'], D[0][j]) for j, idx in enumerate(I[0]) if D[0][j] >= SIMILARITY_THRESHOLD]

if not retrieved:
    print("\nâš ï¸ è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    exit()

# ========================
# ğŸ”¹ 8. æ–‡è„ˆä½œæˆï¼ˆé•·ã•åˆ¶é™ï¼‰
# ========================
MAX_CONTEXT_TOKENS = 800
context_sentences = []
total_tokens = 0
for text, _ in retrieved:
    entry = next((e for e in entries if e["text"] == text), None)
    if entry:
        if total_tokens + entry["tokens"] <= MAX_CONTEXT_TOKENS:
            context_sentences.append(text)
            total_tokens += entry["tokens"]

context = "\n\n".join(context_sentences)

if len(context.strip()) < 50:
    print("\nâš ï¸ æ–‡è„ˆãŒçŸ­ã™ãã¾ã™ã€‚")
    exit()

# ========================
# ğŸ”¹ 9. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
# ========================
prompt = f"""ã‚ãªãŸã¯é³¥ç£æˆ¯ç”»ãƒ»ç”²å·»ã®å°‚é–€AIã§ã™ã€‚
æ¬¡ã®æ–‡è„ˆã«åŸºã¥ã„ã¦è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
æƒ…å ±ãŒãªã‘ã‚Œã°ã€Œãã®ä»¶ã«ã¤ã„ã¦ã¯æ–‡è„ˆã«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡è„ˆ:
{context}

è³ªå•:
{query}

ç­”ãˆï¼š"""

# ========================
# ğŸ”¹ 10. ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´
# ========================
inputs = tokenizer(prompt, truncation=True, max_length=1000, return_tensors="pt")
trimmed_prompt = tokenizer.decode(inputs["input_ids"][0])

# ========================
# ğŸ”¹ 11. å›ç­”ç”Ÿæˆï¼ˆå®‰å®šè¨­å®šï¼‰
# ========================
result = language_model(
    trimmed_prompt,
    max_new_tokens=100,
    do_sample=False,
    pad_token_id=50256
)[0]['generated_text']

# # ========================
# # ğŸ”¹ 12. ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆä»»æ„ï¼‰
# DEBUG = True
# if DEBUG:
#     print("\n--- ğŸ” Debug info ---")
#     for i, (text, score) in enumerate(retrieved):
#         print(f"{i+1}. ({score:.4f}) {text[:60]}...")

# # ========================
# # ğŸ”¹ 13. å‡ºåŠ›
# # ========================
# print("\nğŸ’¡ å›ç­”:")
# print(result)

# ========================
# ğŸ”¹ 12. å¿œç­”ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå´©å£Šå¯¾ç­–ï¼‰
# ========================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
generated_raw = result.replace(trimmed_prompt, "").strip()

# ã€Œç­”ãˆï¼šã€ä»¥é™ã ã‘æŠ½å‡º
if "ç­”ãˆï¼š" in generated_raw:
    generated_clean = generated_raw.split("ç­”ãˆï¼š")[-1].strip()
else:
    generated_clean = generated_raw

# ä¸è‡ªç„¶ãªè¨˜å·ãƒ»ç¹°ã‚Šè¿”ã—ã‚’æ•´ç†
generated_clean = re.sub(r"[ï¼š:\.]{3,}", "ã€‚", generated_clean)
generated_clean = re.sub(r"ã€‚+", "ã€‚", generated_clean).strip()

# æœ€ä½é™ã®æ–‡å­—æ•°ã‚’æº€ãŸã•ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
if len(generated_clean) < 5:
    generated_clean = generated_raw.split("ã€‚")[0] + "ã€‚"

# ========================
# ğŸ”¹ 13. ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ========================
DEBUG = True
if DEBUG:
    print("\n--- ğŸ” Debug info ---")
    for i, (text, score) in enumerate(retrieved):
        print(f"{i+1}. ({score:.4f}) {text[:60]}...")

# ========================
# ğŸ”¹ 14. æœ€çµ‚å‡ºåŠ›
# ========================
print("\nğŸ’¡ å›ç­”:")
print(generated_clean)
